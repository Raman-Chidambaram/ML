{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional Frequency Distribution - 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import brown\n",
    "def calculateCFD(cfdconditions,cfdevents):\n",
    "    stop=stopwords.words('english')\n",
    "\n",
    "    temp = [[genre, word.lower()] for genre in cfdconditions for word in brown.words(categories=genre) if word.lower() not in stop]\n",
    "\n",
    "    cdev_cfd=nltk.ConditionalFreqDist(temp)\n",
    "    cdev_cfd.tabulate(conditions=cfdconditions,samples=cfdevents)\n",
    "\n",
    "    lst=[]\n",
    "    for i in temp:\n",
    "        if i[1].endswith('ing'):\n",
    "            lst.append((i[0],'ing'))\n",
    "\n",
    "        elif i[1].endswith('ed'):\n",
    "            lst.append((i[0],'ed'))\n",
    "\n",
    "    inged_cfd=nltk.ConditionalFreqDist(lst)      \n",
    "    inged_cfd.tabulate(conditions=cfdconditions,samples=['ed','ing'])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    cfdconditions_count = int(input().strip())\n",
    "\n",
    "    cfdconditions = []\n",
    "\n",
    "    for _ in range(cfdconditions_count):\n",
    "        cfdconditions_item = input()\n",
    "        cfdconditions.append(cfdconditions_item)\n",
    "        \n",
    "    cfdevents_count = int(input().strip())\n",
    "\n",
    "    cfdevents = []\n",
    "\n",
    "    for _ in range(cfdevents_count):\n",
    "        cfdevents_item = input()\n",
    "        cfdevents.append(cfdevents_item)\n",
    "\n",
    "    calculateCFD(cfdconditions, cfdevents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming and lemmatize - 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "def performStemAndLemma(textcontent):\n",
    "    pattern = r'\\w+'\n",
    "    tokenizedwords = nltk.regexp_tokenize(textcontent.lower(), pattern)\n",
    "    tokenizedwords_unique = set(tokenizedwords)\n",
    "#    print(tokenizedwords)\n",
    "    stop_words = stopwords.words('english')\n",
    "    filteredwords = [] \n",
    "    for w in tokenizedwords_unique: \n",
    "        if w not in stop_words: \n",
    "            filteredwords.append(w) \n",
    "#    print(filteredwords)\n",
    "    \n",
    "    from nltk import PorterStemmer\n",
    "    porter = PorterStemmer()\n",
    "    porterstemmedwords = [porter.stem(filteredwords) for filteredwords in filteredwords ]\n",
    "    print(sorted(porterstemmedwords))\n",
    "\n",
    "    from nltk import LancasterStemmer\n",
    "    lancaster = LancasterStemmer()\n",
    "    lancasterstemmedwords = [lancaster.stem(filteredwords) for filteredwords in filteredwords ]\n",
    "    print(sorted(lancasterstemmedwords))\n",
    "    \n",
    "    wnl = nltk.WordNetLemmatizer()\n",
    "    lemmatizedwords = [wnl.lemmatize(filteredwords) for filteredwords in filteredwords ]\n",
    "    print(sorted(lemmatizedwords))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    textcontent = input()\n",
    "    performStemAndLemma(textcontent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS Tagging  - 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "def tagPOS(textcontent, taggedtextcontent, defined_tags):\n",
    "\n",
    "    words = nltk.word_tokenize(textcontent)\n",
    "    nltk_pos_tags = nltk.pos_tag(words)\n",
    "    print(nltk_pos_tags)\n",
    "\n",
    "    tagged_pos_tag = [nltk.tag.str2tuple(w) for w in taggedtextcontent.split()]\n",
    "    print(tagged_pos_tag)\n",
    "    \n",
    "    words = nltk.word_tokenize(textcontent)\n",
    "    baseling_tagger = nltk.UnigramTagger(model=defined_tags)\n",
    "    unigram_pos_tag = baseling_tagger.tag(words)\n",
    "    print(unigram_pos_tag)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    textcontent = input()\n",
    "\n",
    "    taggedtextcontent = input()\n",
    "\n",
    "    defined_tags = dict(brown.tagged_words(tagset='universal'))\n",
    "\n",
    "    tagPOS(textcontent, taggedtextcontent, defined_tags)\n",
    "\n",
    "    print(nltk_pos_tags)\n",
    "    print(tagged_pos_tag)\n",
    "    print(unigram_pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing RAW text  - 100%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from urllib import request\n",
    "from collections import Counter\n",
    "def processRawText(textURL):\n",
    "    \n",
    "    textcontent = request.urlopen(textURL).read()\n",
    "    textdecode = textcontent.decode('unicode_escape')\n",
    "    tokenizedlcwords = nltk.word_tokenize(textdecode)    \n",
    "    text1_lcw = [ tokenizedlcwords.lower() for tokenizedlcwords in tokenizedlcwords]\n",
    "    noofwords = len(text1_lcw)\n",
    "    noofunqwords = len(set(text1_lcw))\n",
    "    wordcov =  noofwords / noofunqwords\n",
    " \n",
    "    wordfreq = nltk.FreqDist(text1_lcw)\n",
    "    maxfreqlst = Counter(wordfreq).most_common(3)\n",
    "    maxfreq = maxfreqlst[0][0]\n",
    "    print(noofwords)\n",
    "    print(noofunqwords)\n",
    "    print(int(wordcov))\n",
    "    print(maxfreq)\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    textURL = \"https://hrcdn.net/s3_pub/istreet-assets/2KDELtu3svGwJgNXUXFE7Q/001.txt\"\n",
    "    processRawText(textURL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
